# mlcyber_lab4
Lab 4
REPORT â€“ Lab 4
fx2078
Here, my methodology begins with a thorough evaluation process. Initially, I establish the baseline accuracy and assess attack success rates on the model using the validation split of a clean dataset. Next, I focus on activations at the last pooling layer, consisting of 60 convolution channels. Inspired by Garg et al.'s [Fine Pruning], I implement a pruning technique, systematically removing channels based on their average activation levels. This strategic approach targets the elimination of the least activated channels, crucial for preventing backdoors exploited by attackers. After each iteration of channel pruning, I conduct a careful assessment of validation accuracy. Models falling below a predefined threshold are preserved as benchmarks. The methodology involves creating diverse models, including a Goodnet derived from the original BadNet and a BadNet_pruned. Decision-making relies on the agreement between these models, with discrepancies indicating potential compromises. Results show a consistent pattern: as more channels are pruned, there's a gradual decline in validation accuracy, particularly beyond a certain threshold. This decline is attributed to the removal of inactive channels having negligible impact, while the elimination of active channels significantly impairs the model. The accuracy of the Goodnet mirrors the decline in BadNet_pruned accuracy, and an increase in the attack rate is noted as the Goodnet's accuracy diminishes. This correlation highlights a trade-off between accuracy and susceptibility to attacks, underscoring the delicate balance between robustness and predictive performance in the context of the Goodnet.
<img width="855" alt="image" src="https://github.com/flavjoxhelollari/mlcyber_lab4/assets/47611143/8af90304-78fc-4a67-943a-db9e0c4c9310">
<img width="869" alt="image" src="https://github.com/flavjoxhelollari/mlcyber_lab4/assets/47611143/f76910cd-9183-4791-8410-eee3bc40f616">
